@article{nash1950equilibrium,
  title={Equilibrium points in n-person games},
  author={Nash Jr, John F},
  journal={Proceedings of the national academy of sciences},
  volume={36},
  number={1},
  pages={48--49},
  year={1950},
  publisher={National Acad Sciences}
}

@article{kober2013reinforcement,
  title={Reinforcement learning in robotics: A survey},
  author={Kober, Jens and Bagnell, J Andrew and Peters, Jan},
  journal=IJRR,
  volume={32},
  number={11},
  pages={1238--1274},
  year={2013},
  publisher={SAGE Publications Sage UK: London, England}
}

% basic algorithms
@inproceedings{levine2013guided,
  title={Guided policy search},
  author={Levine, Sergey and Koltun, Vladlen},
  booktitle=ICML,
  pages={1--9},
  year={2013},
  organization={PMLR}
}

@inproceedings{deisenroth2011pilco,
  title={{PILCO}: A model-based and data-efficient approach to policy search},
  author={Deisenroth, Marc and Rasmussen, Carl E},
  booktitle=ICML,
  pages={465--472},
  year={2011}
}

% books

@article{sutton1992reinforcement,
  title={Reinforcement learning is direct adaptive optimal control},
  author={Sutton, Richard S and Barto, Andrew G and Williams, Ronald J},
  journal=IEEE_M_CS,
  volume={12},
  number={2},
  pages={19--22},
  year={1992},
  publisher={IEEE}
}

@article{busoniu2008comprehensive,
  title={A comprehensive survey of multiagent reinforcement learning},
  author={Busoniu, Lucian and Babuska, Robert and De Schutter, Bart},
  journal=IEEE_J_SMCC,
  volume={38},
  number={2},
  pages={156--172},
  year={2008}
}

@article{panait2005cooperative,
  title={Cooperative multi-agent learning: The state of the art},
  author={Panait, Liviu and Luke, Sean},
  journal={Autonomous agents and multi-agent systems},
  volume={11},
  number={3},
  pages={387--434},
  year={2005},
  publisher={Kluwer Academic Publishers}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{li2019distributed,
  title={Distributed Reinforcement Learning for Decentralized Linear Quadratic Control: A Derivative-Free Policy Optimization Approach},
  author={Li, Yingying and Tang, Yujie and Zhang, Runyu and Li, Na},
  journal={arXiv preprint arXiv:1912.09135},
  year={2019}
}

@article{bertsekas2020multiagent,
  title={Multiagent Value Iteration Algorithms in Dynamic Programming and Reinforcement Learning},
  author={Bertsekas, Dimitri},
  journal={arXiv preprint arXiv:2005.01627},
  year={2020}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@article{silver2016mastering,
  title={Mastering the game of {G}o with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{piot2016bridging,
  title={Bridging the gap between imitation learning and inverse reinforcement learning},
  author={Piot, Bilal and Geist, Matthieu and Pietquin, Olivier},
  journal={IEEE transactions on neural networks and learning systems},
  volume={28},
  number={8},
  pages={1814--1826},
  year={2016},
  publisher={IEEE}
}

@inproceedings{silver2014deterministic,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  year={2014}
}

@article{coraluppi1999risk,
  title={Risk-sensitive and minimax control of discrete-time, finite-state Markov decision processes},
  author={Coraluppi, Stefano P and Marcus, Steven I},
  journal={Automatica},
  volume={35},
  number={2},
  pages={301--309},
  year={1999},
  publisher={Elsevier}
}

@article{ono2015chance,
  title={Chance-constrained dynamic programming with application to risk-aware robotic space exploration},
  author={Ono, Masahiro and Pavone, Marco and Kuwata, Yoshiaki and Balaram, J},
  journal=AR,
  volume={39},
  pages={555--571},
  year={2015},
  publisher={Springer}
}


% see also sartoretti2019primal

@article{bertsekas2022newton,
  title={Newtonâ€™s method for reinforcement learning and model predictive control},
  author={Bertsekas, Dimitri},
  journal={Results in Control and Optimization},
  volume={7},
  pages={100121},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{ng2003autonomous,
  title={Autonomous helicopter flight via reinforcement learning.},
  author={Ng, Andrew Y and Kim, H Jin and Jordan, Michael I and Sastry, Shankar and Ballianda, Shiv},
  booktitle=NIPS,
  volume={16},
  year={2003},
  organization={Citeseer}
}

@inproceedings{rahmatizadeh2018vision,
  title={Vision-based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration},
  author={Rahmatizadeh, Rouhollah and Abolghasemi, Pooya and B{\"o}l{\"o}ni, Ladislau and Levine, Sergey},
  booktitle=IEEE_C_ICRA,
  pages={3758--3765},
  year={2018},
  organization={IEEE}
}

%% DQN
@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group},
  annote={original DQN paper}
}

@inproceedings{fan2020theoretical,
  title={A theoretical analysis of deep Q-learning},
  author={Fan, Jianqing and Wang, Zhaoran and Xie, Yuchen and Yang, Zhuoran},
  booktitle=LfDC,
  pages={486--489},
  year={2020},
  organization={PMLR},
  annote={replay buffer is similar as sampling from independent distribution, target network bounds the variance of the estimated gradient}
}

% representation learning
@inproceedings{uehara2021representation,
  title={Representation Learning for Online and Offline {RL} in Low-rank {MDPs}},
  author={Uehara, Masatoshi and Zhang, Xuezhou and Sun, Wen},
  booktitle=CORL,
  year={2021},
  annote={Low rank factorization, see talk 24-CISE-Xuezhou Zhang.org}
}

@inproceedings{guo2023provably,
  title={Provably Efficient Representation Learning with Tractable Planning in Low-Rank POMDP},
  author={Guo, Jiacheng and Li, Zihao and Wang, Huazheng and Wang, Mengdi and Yang, Zhuoran and Zhang, Xuezhou},
  booktitle=ICML,
  year={2023},
  annote={Low rank factorization, see talk 24-CISE-Xuezhou Zhang.org}
}

@article{bengio2013representation,
  title={Representation learning: A review and new perspectives},
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal=IEEE_J_PAMI,
  volume={35},
  number={8},
  pages={1798--1828},
  year={2013},
  publisher={IEEE}
}

@article{scholkopf2021toward,
  title={Toward causal representation learning},
  author={Sch{\"o}lkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  journal=IEEE_J_PROC,
  volume={109},
  number={5},
  pages={612--634},
  year={2021},
  publisher={IEEE}
}

% see also pari2021surprising
