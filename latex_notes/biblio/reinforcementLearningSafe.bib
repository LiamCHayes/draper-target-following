% Surveys
@inproceedings{pecka2014safe,
  title={Safe exploration techniques for reinforcement learning--an overview},
  author={Pecka, Martin and Svoboda, Tomas},
  booktitle={International Workshop on Modelling and Simulation for Autonomous Systems},
  pages={357--375},
  year={2014},
  organization={Springer}
}

@article{garcia2015comprehensive,
  title={A comprehensive survey on safe reinforcement learning},
  author={Garc{\i}a, Javier and Fern{\'a}ndez, Fernando},
  journal={Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={1437--1480},
  year={2015}
}

% Constrainted
@article{paternain2019constrained,
  title={Constrained reinforcement learning has zero duality gap},
  author={Paternain, Santiago and Chamon, Luiz FO and Calvo-Fullana, Miguel and Ribeiro, Alejandro},
  journal={arXiv preprint arXiv:1910.13393},
  year={2019}
}

@inproceedings{chow2018lyapunov,
  title={A lyapunov-based approach to safe reinforcement learning},
  author={Chow, Yinlam and Nachum, Ofir and Duenez-Guzman, Edgar and Ghavamzadeh, Mohammad},
  booktitle=NIPS,
  pages={8103--8112},
  year={2018}
}

@inproceedings{regan2009regret,
  title={Regret-based reward elicitation for Markov decision processes},
  author={Regan, Kevin and Boutilier, Craig},
  booktitle={Conference on Uncertainty in Artificial Intelligence},
  pages={444--451},
  year={2009}
}

@book{altman1999constrained,
  title={Constrained Markov decision processes},
  author={Altman, Eitan},
  volume={7},
  year={1999},
  publisher={CRC Press}
}

@article{altman1998constrained,
  title={Constrained Markov decision processes with total cost criteria: Lagrangian approach and dual linear program},
  author={Altman, Eitan},
  journal={Mathematical methods of operations research},
  volume={48},
  number={3},
  pages={387--417},
  year={1998},
  publisher={Springer}
}

@article{geibel2005risk,
  title={Risk-sensitive reinforcement learning applied to control under constraints},
  author={Geibel, Peter and Wysotzki, Fritz},
  journal={Journal of Artificial Intelligence Research},
  volume={24},
  pages={81--108},
  year={2005}
}

@inproceedings{el2016convex,
  title={Convex synthesis of randomized policies for controlled Markov chains with density safety upper bound constraints},
  author={El Chamie, Mahmoud and Yu, Yue and A{\c{c}}{\i}kme{\c{s}}e, Beh{\c{c}}et},
  booktitle={2016 American Control Conference (ACC)},
  pages={6290--6295},
  notes={Similar to altman1998constrained, but uses step-wise min-max LP to also take into account transitory response},
  year=2016,
  organization={IEEE}
}

@article{dalal2018safe,
  title={Safe exploration in continuous action spaces},
  author={Dalal, Gal and Dvijotham, Krishnamurthy and Vecerik, Matej and Hester, Todd and Paduraru, Cosmin and Tassa, Yuval},
  journal={arXiv preprint arXiv:1801.08757},
  year={2018}
}

@inproceedings{gabor1998multi,
  title={Multi-criteria reinforcement learning.},
  author={G{\'a}bor, Zolt{\'a}n and Kalm{\'a}r, Zsolt and Szepesv{\'a}ri, Csaba},
  booktitle={ICML},
  volume={98},
  pages={197--205},
  year={1998},
  organization={Citeseer}
}

@article{mossalam2016multi,
  title={Multi-objective deep reinforcement learning},
  author={Mossalam, Hossam and Assael, Yannis M and Roijers, Diederik M and Whiteson, Shimon},
  journal={arXiv preprint arXiv:1610.02707},
  year={2016}
}

@article{roijers2015computing,
  title={Computing convex coverage sets for faster multi-objective coordination},
  author={Roijers, Diederik Marijn and Whiteson, Shimon and Oliehoek, Frans A},
  journal={Journal of Artificial Intelligence Research},
  volume={52},
  pages={399--443},
  year={2015}
}

@article{roijers2013survey,
  title={A survey of multi-objective sequential decision-making},
  author={Roijers, Diederik M and Vamplew, Peter and Whiteson, Shimon and Dazeley, Richard},
  journal={Journal of Artificial Intelligence Research},
  volume={48},
  pages={67--113},
  year={2013}
}

@inproceedings{achiam2017constrained,
  title={Constrained policy optimization},
  author={Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={22--31},
  year={2017},
  organization={PMLR}
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}

@inproceedings{kakade2002approximately,
  title={Approximately Optimal Approximate Reinforcement Learning},
  author={Kakade, Sham and Langford, John},
  booktitle={Proceedings of the Nineteenth International Conference on Machine Learning},
  pages={267--274},
  year={2002}
}

@inproceedings{pirotta2013safe,
  title={Safe policy iteration},
  author={Pirotta, Matteo and Restelli, Marcello and Pecorino, Alessio and Calandriello, Daniele},
  booktitle={International Conference on Machine Learning},
  pages={307--315},
  year={2013},
  organization={PMLR}
}

@inproceedings{qin2021density,
  title={Density constrained reinforcement learning},
  author={Qin, Zengyi and Chen, Yuxiao and Fan, Chuchu},
  booktitle=ICML,
  pages={8682--8692},
  year={2021},
  organization={PMLR}
}

@inproceedings{dawson2021safe,
  title={Safe Nonlinear Control Using Robust Neural Lyapunov-Barrier Functions},
  author={Dawson, Charles and Qin, Zengyi and Gao, Sicun and Fan, Chuchu},
  booktitle=CORL,
  year={2021}
}

@inproceedings{qin2020learning,
  title={Learning Safe Multi-agent Control with Decentralized Neural Barrier Certificates},
  author={Qin, Zengyi and Zhang, Kaiqing and Chen, Yuxiao and Chen, Jingkai and Fan, Chuchu},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{sun2020learning,
  title={Learning certified control using contraction metric},
  author={Sun, Dawei and Jha, Susmit and Fan, Chuchu},
  journal={arXiv preprint arXiv:2011.12569},
  year={2020}
}

% Applications

@article{mastronarde2011fast,
  title={Fast reinforcement learning for energy-efficient wireless communication},
  author={Mastronarde, Nicholas and van der Schaar, Mihaela},
  journal=IEEE_J_SP,
  volume={59},
  number={12},
  pages={6262--6266},
  year={2011},
  note={Uses fact that system dynamics is the product of a known component with an unknown component},
  publisher={IEEE}
}

@inproceedings{junges2016safety,
  title={Safety-constrained reinforcement learning for MDPs},
  author={Junges, Sebastian and Jansen, Nils and Dehnert, Christian and Topcu, Ufuk and Katoen, Joost-Pieter},
  booktitle={International Conference on Tools and Algorithms for the Construction and Analysis of Systems},
  pages={130--146},
  year={2016},
  organization={Springer}
}

@techreport{gordon1999approximate,
  title={Approximate solutions to Markov decision processes},
  author={Gordon, Geoffrey J},
  year={1999},
  institution={CARNEGIE-MELLON UNIV PITTSBURGH PA SCHOOL OF COMPUTER SCIENCE}
}

%J. Garc´ıa and F. Fern´andez, “A Comprehensive Survey on Safe Reinforcement Learning,” Journal of Machine Learning Research, 2015.
%[9] R. Cheng, G. Orosz, R. M. Murray, and J. W. Burdick, “End-to-End Safe Reinforcement Learning through Barrier Functions for Safety-Critical Continuous Control Tasks,” in Proc. AAAI, 2019
