@misc{tron_paper,
  title = {A Control Barrier Function Candidate for Quadrotors with Limited
           Field of View},
  author = {Biagio Trimarchi and Fabrizio Schiano and Roberto Tron},
  year = {2025},
  eprint = {2410.01277},
  archivePrefix = {arXiv},
  primaryClass = {eess.SY},
  url = {https://arxiv.org/abs/2410.01277},
}

@article{multi_target_clustered,
  author = {Chen, Jun and Dames, Philip and Park, Shinkyu},
  year = {2025},
  month = {05},
  pages = {},
  title = {Effective tracking of unknown clustered targets using a distributed
           team of mobile robots},
  volume = {49},
  journal = {Autonomous Robots},
  doi = {10.1007/s10514-025-10200-z},
}

@article{reinforcement_learning,
  title = {Enhancing continuous control of mobile robots for end-to-end visual
           active tracking},
  journal = {Robotics and Autonomous Systems},
  volume = {142},
  pages = {103799},
  year = {2021},
  issn = {0921-8890},
  doi = {https://doi.org/10.1016/j.robot.2021.103799},
  url = {https://www.sciencedirect.com/science/article/pii/S0921889021000841},
  author = {Alessandro Devo and Alberto Dionigi and Gabriele Costante},
  keywords = {Visual active tracking, Deep learning for robotic applications,
              Reinforcement learning},
  abstract = {In the last decades, visual target tracking has been one of the
              primary research interests of the Robotics research community. The
              recent advances in Deep Learning technologies have made the
              exploitation of visual tracking approaches effective and possible
              in a wide variety of applications, ranging from automotive to
              surveillance and human assistance. However, the majority of the
              existing works focus exclusively on passive visual tracking, i.e.,
              tracking elements in sequences of images by assuming that no
              actions can be taken to adapt the camera position to the motion of
              the tracked entity. On the contrary, in this work, we address
              visual active tracking, in which the tracker has to actively search
              for and track a specified target. Current State-of-the-Art
              approaches use Deep Reinforcement Learning (DRL) techniques to
              address the problem in an end-to-end manner. However, two main
              problems arise: (i) most of the contributions focus only on
              discrete action spaces, and the ones that consider continuous
              control do not achieve the same level of performance; and (ii) if
              not properly tuned, DRL models can be challenging to train,
              resulting in considerably slow learning progress and poor final
              performance. To address these challenges, we propose a novel
              DRL-based visual active tracking system that provides continuous
              action policies. To accelerate training and improve the overall
              performance, we introduce additional objective functions and a
              Heuristic Trajectory Generator (HTG) to facilitate learning.
              Through extensive experimentation, we show that our method can
              reach and surpass other State-of-the-Art approaches performances,
              and demonstrate that, even if trained exclusively in simulation, it
              can successfully perform visual active tracking even in real
              scenarios.},
}

@misc{fov_line_of_sight_paper,
  title = {Control Strategies for Pursuit-Evasion Under Occlusion Using
           Visibility and Safety Barrier Functions},
  author = {Minnan Zhou and Mustafa Shaikh and Vatsalya Chaubey and Patrick
            Haggerty and Shumon Koga and Dimitra Panagou and Nikolay Atanasov},
  year = {2025},
  eprint = {2411.01321},
  archivePrefix = {arXiv},
  primaryClass = {cs.RO},
  url = {https://arxiv.org/abs/2411.01321},
}

@article{observer_based_paper,
  title = {A Control Barrier Function Approach for Observer-based Visually Safe
           Pursuit Control with Spherical Obstacles*},
  journal = {IFAC-PapersOnLine},
  volume = {56},
  number = {2},
  pages = {10799-10804},
  year = {2023},
  note = {22nd IFAC World Congress},
  issn = {2405-8963},
  doi = {https://doi.org/10.1016/j.ifacol.2023.10.751},
  url = {https://www.sciencedirect.com/science/article/pii/S240589632301128X},
  author = {Tesshu Fujinami and Junya Yamauchi and Riku Funada and Masayuki
            Fujita},
  keywords = {Vision-based Control, Mobile robots, Nonlinear observers, Control
              barrier functions, Real time optimization and control},
  abstract = {Pursuing a target in the presence of obstacles requires that an
              autonomous mobile robot keeps sight of the target in a way robust
              to the target's unknown behavior. This paper presents visually safe
              pursuit control, which keeps a target with the unknown motion
              inside the camera's field of view while preventing occlusion caused
              by spherical obstacles. Framed as forward invariance of sets in the
              SE(3) state space, visual safety is ensured by the Control Barrier
              Functions (CBFs) approach. Concretely, by showing the
              Input-to-State stability of the vision-based observer that
              estimates the target's motion, we design safety certificates for
              visual safety that accommodate uncertainties in the target's
              motion. This enables us to synthesize a safe controller as a
              Quadratic Programming problem. Finally, the theoretical results are
              verified via a simulation of a visual pursuit scenario.},
}

@inproceedings{multi_robot_paper,
  author = {Won, Seung-Beom and Ahn, Hyo-Sung},
  booktitle = {2024 24th International Conference on Control, Automation and
               Systems (ICCAS)},
  title = {Vision-based Formation Control with Control Barrier Function},
  year = {2024},
  volume = {},
  number = {},
  pages = {478-482},
  keywords = {Visualization;Surveillance;Robot vision systems;Cameras;Formation
              control;Real-time systems;Trajectory;Maintenance;Quadratic
              programming;Mobile robots;Image-based visual servoing;Visibility
              maintenance;Control barrier function},
  doi = {10.23919/ICCAS63016.2024.10773299},
}

@article{mobile_manipulator_paper,
  title = {Control barrier function based visual servoing for Mobile Manipulator
           Systems under functional limitations},
  journal = {Robotics and Autonomous Systems},
  volume = {182},
  pages = {104813},
  year = {2024},
  issn = {0921-8890},
  doi = {https://doi.org/10.1016/j.robot.2024.104813},
  url = {https://www.sciencedirect.com/science/article/pii/S0921889024001970},
  author = {Shahab Heshmati-Alamdari and Maryam Sharifi and George C. Karras and
            George K. Fourlas},
  keywords = {Robust vision-based control, Mobile Manipulator System control,
              Image-based visual servoing, Control barrier functions},
  abstract = {This paper proposes a new control strategy for Mobile Manipulator
              Systems (MMSs) that integrates image-based visual servoing (IBVS)
              to address operational limitations and safety constraints. The
              proposed approach based on the concept of control barrier functions
              (CBFs), provides a solution to address various operational
              challenges including visibility constraints, manipulator joint
              limits, predefined system velocity bounds, and system dynamic
              uncertainties. The proposed control strategy is a two-tiered
              structure, wherein the first level, a CBF-IBVS controller
              calculates control commands, taking into account the Field of View
              (FoV) constraints. By leveraging null space techniques, these
              commands are transposed to the joint-level configuration of the MMS
              , while considering system operational limits. Subsequently, in the
              second level, a CBF velocity controller employed for the entire MMS
              undertakes the tracking of the commands at the joint level,
              ensuring compliance with the predefined system’s velocity
              limitations as well as the safety of the whole combined system
              dynamics. The proposed control strategy offers superior transient
              and steady-state responses and heightened resilience to
              disturbances and modeling uncertainties. Furthermore, due to its
              low computational complexity, it can be easily implemented on an
              onboard computing system, facilitating real-time operation. The
              proposed strategy’s effectiveness is illustrated via simulation
              outcomes, which reveal enhanced performance and system safety
              compared to conventional IBVS methods. The results indicate that
              the proposed approach is effective in addressing the challenging
              operational limitations and safety constraints of mobile
              manipulator systems, making it suitable for practical applications.
              },
}

@misc{learned_cbf_paper,
  title = {Neural Control Barrier Functions for Safe Navigation},
  author = {Marvin Harms and Mihir Kulkarni and Nikhil Khedekar and Martin
            Jacquet and Kostas Alexis},
  year = {2024},
  eprint = {2407.19907},
  archivePrefix = {arXiv},
  primaryClass = {cs.RO},
  url = {https://arxiv.org/abs/2407.19907},
}

@inproceedings{direct_control_paper,
  author = {Dias, Duarte and Lima, Pedro Urbano and Martinoli, Alcherio},
  title = {Distributed Formation Control of Quadrotors under Limited Sensor
           Field of View},
  year = {2016},
  isbn = {9781450342391},
  publisher = {International Foundation for Autonomous Agents and Multiagent
               Systems},
  address = {Richland, SC},
  abstract = {This work tackles the problem of quadrotor formation control,
              using exclusively on-board resources. Local inter-robot
              localization systems are typically characterized by limited sensing
              capabilities, either in range or in the field of view. Most of the
              existing literature on the subject uses inter-robot communication
              to obtain the unavailable information, but problems such as
              communication delays and packet loss can seriously compromise the
              system stability, especially when the system shows fast dynamics
              such as that of quadrotors. This work focuses on the sensor field
              of view limitation: it proposes a formation control algorithm that
              extends the existing methods to allow each quadrotor to control the
              occupied area of its sensor field of view, while moving to the
              right place in the formation. This decreases the situations when
              necessary inter-robot information is unavailable through local
              sensing, thus reducing the communication requirements. The system
              is proven to be stable when this algorithm is applied. Results,
              both using simulated and real quadrotors, show the correct behavior
              of the algorithm without the use of communications, even when each
              robot can only sense a subset of the robots in the group.},
  booktitle = {Proceedings of the 2016 International Conference on Autonomous
               Agents \& Multiagent Systems},
  pages = {1087–1095},
  numpages = {9},
  keywords = {formation control, limited field of view, quadrotors},
  location = {Singapore, Singapore},
  series = {AAMAS '16},
}

@inproceedings{rrt_paper,
  author = {Lu, Wenjie and Zhang, Guoxian and Ferrari, Silvia},
  booktitle = {49th IEEE Conference on Decision and Control (CDC)},
  title = {A randomized hybrid system approach to coordinated robotic sensor
           planning},
  year = {2010},
  volume = {},
  number = {},
  pages = {3857-3864},
  keywords = {Robot sensing systems;Robot kinematics;Collision
              avoidance;Geometry;Path planning;Planning},
  doi = {10.1109/CDC.2010.5717351},
}

@article{visual_servo_paper,
  author = {Jing Xin and Han Cheng and Baojing Ran},
  title = {Visual servoing of robot manipulator with weak field-of-view
           constraints},
  journal = {International Journal of Advanced Robotic Systems},
  volume = {18},
  number = {1},
  pages = {1729881421990320},
  year = {2021},
  doi = {10.1177/1729881421990320},
  URL = {https://doi.org/10.1177/1729881421990320},
  eprint = {https://doi.org/10.1177/1729881421990320},
  abstract = { Aiming at the problem of servoing task failure caused by the
              manipulated object deviating from the camera field-of-view (FOV)
              during the robot manipulator visual servoing (VS) process, a new VS
              method based on an improved tracking learning detection (TLD)
              algorithm is proposed in this article, which allows the manipulated
              object to deviate from the camera FOV in several continuous frames
              and maintains the smoothness of the robot manipulator motion during
              VS. Firstly, to implement the robot manipulator visual object
              tracking task with strong robustness under the weak FOV constraints
              , an improved TLD algorithm is proposed. Then, the algorithm is
              used to extract the image features (object in the camera FOV) or
              predict image features (object out of the camera FOV) of the
              manipulated object in the current frame. And then, the position of
              the manipulated object in the current image is further estimated.
              Finally, the visual sliding mode control law is designed according
              to the image feature errors to control the motion of the robot
              manipulator so as to complete the visual tracking task of the robot
              manipulator to the manipulated object in complex natural scenes
              with high robustness. Several robot manipulator VS experiments were
              conducted on a six-degrees-of-freedom MOTOMANSV3 industrial
              manipulator under different natural scenes. The experimental
              results show that the proposed robot manipulator VS method can
              relax the FOV constraint requirements on real-time visibility of
              manipulated object and effectively solve the problem of servoing
              task failure caused by the object deviating from the camera FOV
              during the VS. },
}

@article{learned_motion_model,
    title={A Review of Deep Learning-Based Vehicle Motion Prediction for Autonomous Driving},
    author={Huang, Renbo and Zhuo, Guirong and Xiong, Lu and Lu, Shouyi and Tian, Wei},
    volume={15},
    url={http://dx.doi.org/10.3390/su152014716},
    doi={10.3390/su152014716},
    number={20},
    journal={Sustainability},
    publisher={MDPI AG},
    year={2023},
    month={Oct},
    pages={14716},
    language={en},
}

@article{aise,
title = {Towards Resilient Tracking in Autonomous Vehicles: A Distributionally Robust Input and State Estimation Approach},
journal = {IFAC-PapersOnLine},
volume = {59},
number = {3},
pages = {31-36},
year = {2025},
note = {12th IFAC Symposium on Intelligent Autonomous Vehicles IAV 2025},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2025.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S2405896325003453},
author = {Kasra Azizi and Kumar Anurag and Wenbin Wan},
keywords = {System identification, Robust estimation, Estimation, fault detection},
abstract = {This paper proposes a novel framework for the distributionally robust input and state estimation (DRISE) for autonomous vehicles operating under model uncertainties and measurement outliers. The proposed framework improves the input and state estimation (ISE) approach by integrating distributional robustness, enhancing the estimator’s resilience and robustness to adversarial inputs and unmodeled dynamics. Moment-based ambiguity sets capture probabilistic uncertainties in both system dynamics and measurement noise, offering analytical tractability and efficiently handling uncertainties in mean and covariance. In particular, the proposed framework minimizes the worst-case estimation error, ensuring robustness against deviations from nominal distributions. The effectiveness of the proposed approach is validated through simulations conducted in the CARLA autonomous driving simulator, demonstrating improved performance in state estimation accuracy and robustness in dynamic and uncertain environments.}
}

@misc{factor_graph,
      title={Vehicle State Estimation through Modular Factor Graph-based Fusion of Multiple Sensors}, 
      author={Pragyan Dahal and Jai Prakash and Stefano Arrigoni and Francesco Braghin},
      year={2023},
      eprint={2308.04847},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2308.04847}, 
}
